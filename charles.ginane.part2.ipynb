{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Régularisation de problèmes mal posés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préambule\n",
    "\n",
    "### Un problème mal posé ?\n",
    "\n",
    "Qu'est-ce-qu'un problème mal posé ? Pour commencer, nous allons définir qu'est ce qu'un problème bien posé. Un problème ($P$) bien posé dit respecter les 3 conditions suivantes:\n",
    "* Une solution au problème $P$ existe\n",
    "* La solution est unique\n",
    "* La solution dépend continuellement de la donnée\n",
    "\n",
    "Un problème mal posé est un problème ($P$) qui ne respecte pas une des conditions situées au dessus. Il est donc difficile de trouver une solution aux problèmes mal posés avec les méthodes vues auparavant. Pour trouver une solution optimale à ce type de problème nous allons utiliser la méthode analytique des moindres carrés.\n",
    "Nous utiliserons aussi les méthodes de régularisation telles que LASSO ou Tikhonov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.9\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principe\n",
    "\n",
    "#### Principe - cas général\n",
    "\n",
    "Le but de la résolution de la méthode analytique des moindres de carrés est de pouvoir trouver une solution permettant de trouver une relation entre des groupes différents en ajoutant de l'information dans ces données.[BV04]\n",
    "\n",
    "Plus en détails, on recherche le vecteur $y \\in \\mathbb{R}^N$ qui contient les éléments {$y_1, y_2, ..., y_n$} où $y_i \\in R$. Le vecteur $y$ résulte d'une corrélation des données de la matrice $X$ qui est une matrice $N \\times P$ où $P$ représente le nombre de paramètres des éléments $x$. Nous allons en déduire $y$ à l'aide de la formule suivante:\n",
    "$$\n",
    "y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "où\n",
    "* $\\beta$ est vecteur de paramètre qui rajoute l'information dans les données X.\n",
    "* $\\epsilon$ qui représente les résidus (différence entre le point et la droite $y$) aussi appeler erreur de prédiction.\n",
    "\n",
    "On peut calculer la SSE (Sum of Squared Errors) qui représente la somme de tous les résidus.\n",
    "\n",
    "$$\n",
    "SSE(\\beta) = \\sum_{i}^{N}((y_i - x_i^T\\beta)^2)\\\\\n",
    "SSE(\\beta) = (y - X\\beta)^T(y - X\\beta)\\\\\n",
    "SSE(\\beta) = ||y - X\\beta||^2_2\n",
    "$$\n",
    "\n",
    "La $SSE$ va nous permettre d'appliquer des pénalités à nos données afin de trouver la solution optimale à notre problème.\n",
    "\n",
    "#### Exemple avec la régression linéaire à 2 dimensions\n",
    "\n",
    "Soit deux ensembles $E_1$ et $E_2$ de taille $n$ où $\\{x_1, x_2, ..., x_n\\}$ et $\\{y_1, y_2, ... , y_n\\}$. $E_1$ et $E_2$ représente des points dans un domaine à 2 dimensions.\n",
    "\n",
    "on souhaite construire une droite qui séparerait au mieux les deux ensembles $E_1$ et $E_2$.\n",
    "\n",
    "La méthode des moindres carrés nous permet d'obtenir une droite d'équation du type $y = ax + b$.\n",
    "\n",
    "Comme montrer au dessus, nous allons calculer la SSE de nos points. La SSE va nous permettre de retrouver les coefficients $a$ et $b$ ppur notre droite objectif $y$.\n",
    "Ci-dessous la SSE:\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i = 0}^{n}{(y_i - (ax_i + b))^2}\n",
    "$$\n",
    "\n",
    "Afin d'obtenir la droite pour séparer nos éléments, il faut trouver les valeurs de $a$ et de $b$ à l'aide de la formule de la $SSE$.\n",
    "\n",
    "#### Démonstration\n",
    "\n",
    "**Cherchons b**\n",
    "\n",
    "On a $$SCE = \\sum_{i = 0}^{n}{(y_i - (ax_i + b))^2} = \\sum_{i = 0}^{n}{(-ax_i - b + y_i)^2}$$\n",
    "\n",
    "Nous calculons la dérivée partielle en fonction de $b$. Nous considérons $a$ comme une constante\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{SSE}}{\\partial{b}} = 0 \\iff -2 \\sum_{i = 0}^{n}{(-ax_i - b + y_i)} = 0\n",
    "$$\n",
    "\n",
    "Nous séparons les deux termes de la somme:\n",
    "\n",
    "$$\n",
    "\\iff \\sum_{i = 0}^{n}{(-ax_i - b)} + \\sum_{i = 0}^{n}{y_i} = 0\\\\\n",
    "\\iff \\sum_{i = 0}^{n}{(-ax_i)} - \\sum_{i = 0}^{n}{b} + \\sum_{i = 0}^{n}{y_i} = 0\\\\\n",
    "\\iff \\sum_{i = 0}^{n}{(-ax_i)} - nb + \\sum_{i = 0}^{n}{y_i} = 0\\\\\n",
    "\\iff -nb = a\\sum_{i=0}^{n}{x_i} - \\sum_{i = 0}^{n}{y_i}\\\\\n",
    "\\iff b = -a\\frac{\\sum{x_i}}{n} + \\frac{\\sum{y_i}}{n}\n",
    "$$\n",
    "\n",
    "On sait que $\\frac{\\sum{x_i}}{n}$ équivaut à la moyenne de x donc $\\frac{\\sum{x_i}}{n} = \\overline{x}$. De même pour $\\frac{\\sum{y_i}}{n} = \\overline{y}$. Donc:\n",
    "\n",
    "$$\n",
    "b = -a\\overline{x} + \\overline{y}\n",
    "$$\n",
    "\n",
    "**Cherchons a**\n",
    "\n",
    "Avant de calculer la dérivée partielle de $SSE$ en fonction de $a$, nous allons remplacer $b$ dans l'expression de $SSE$\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i = 0}^{n}{(-ax_i + a\\overline{x} - \\overline{y} + y_i)^2}\\\\\n",
    "SSE = \\sum_{i = 0}^{n}{(-a(x_i - \\overline{x}) + (y_i - \\overline{y}))^2}\n",
    "$$\n",
    "\n",
    "On reconnaît une identité remarquable du type $(a-b)^2 = a^2 - 2ab + b^2$\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=0}^{n}({a^2(x_i - \\overline{x})^2}) + 2\\sum_{i=0}^{n}({-a(x_i - \\overline{x})(y_i - \\overline{y})}) + \\sum_{i=0}^{n}(y_i - \\overline{y})^2\n",
    "$$\n",
    "\n",
    "On résout la dérivée partielle de la $SCE$ en fonction de $a$ telle que $\\frac{\\partial{SCE}}{\\partial{a}} = 0$\n",
    "\n",
    "$$\n",
    "\\iff \\sum_{i=0}^{n}({2a(x_i - \\overline{x})^2}) - 2 \\sum_{i=0}^{n}({(x_i - \\overline{x})(y_i - \\overline{y})})\\\\\n",
    "\\iff a = \\frac{2\\sum_{i=0}^{n}({(x_i - \\overline{x})(y_i - \\overline{y})}}{2\\sum_{i=0}^{n}({(x_i - \\overline{x})^2})}\\\\\n",
    "$$\n",
    "\n",
    "Or $\\sum_{i=0}^{n}({(x_i - \\overline{x})(y_i - \\overline{y})} = \\sigma_{xy}$ et $\\sum_{i=0}^{n}({(x_i - \\overline{x})} = \\sigma_x$ donc\n",
    "\n",
    "$$\n",
    "a = \\frac{\\sigma_{xy}}{\\sigma_{x}^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.10\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préambule\n",
    "\n",
    "Comme vu à la question 2.9, la méthode des moindres carrés permet de trouver une corrélation entre des ensembles différents. Nous avons vu la notion de $SSE$ comme étant la somme des résidus. Afin de pouvoir trouver la séparation optimale, il faut trouver un moyen de minimiser la $SSE$. Nous allons utiliser des méthodes de régularisation et de pénalité que nous allons appliquer sur la $SSE$. Au cours de cette question, nous allons présenter les méthodes de régularisation Tikhonov et de LASSO.\n",
    "\n",
    "On rappelle la $SSE$:\n",
    "\n",
    "$$\n",
    "SSE(\\beta) = ||y - X\\beta||^2_2\n",
    "$$\n",
    "\n",
    "où\n",
    "* $\\beta$ est notre ensemble\n",
    "* $y$ notre fonction objective\n",
    "* $X$ les points\n",
    "\n",
    "On définit une pénalité dans le domaine du machine learning (ML) par la minisation de la formule suivante:\n",
    "\n",
    "$$\n",
    "arg min \\ Pen(\\beta) = \\mathcal{L}(\\beta) + \\lambda \\Omega(x)\n",
    "$$\n",
    "\n",
    "où\n",
    "* $\\mathcal{L}(\\beta)$ est la fonction de regression (fonction perte en ML)\n",
    "* $\\lambda$ est le paramètre de régularisation\n",
    "* $\\Omega(\\beta)$ est la fonction de pénalité\n",
    "\n",
    "Les deux méthodes présentent en dessous utilisent deux fonctions de pénalité différentes avec laquelle nous pourrons trouver le vecteur $\\beta$ le plus optimal possible à notre problème.\n",
    "\n",
    "### Les méthodes\n",
    "#### Tikhonov\n",
    "\n",
    "Tikhonov (de son vrai non Andreï Nikolaïevitch Tikhonov) est un mathématicien russe. Il est connu pour avoir prouvé la régularisation qui porte son nom. Le but de cette régularisation est d'appliquer une régularisation $\\mathcal{L}_2$ comme fonction de pénalité. Cette régularisation s'appelle aussi Ridge[TiK43].\n",
    "\n",
    "$$\n",
    "Ridge(\\beta) = ||y-X\\beta||^{2}_{2} - \\lambda ||\\beta||^2_2\n",
    "$$\n",
    "\n",
    "Afin de pouvoir trouver le $\\beta$ minimal, il suffit de résoudre:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{Ridge}}{\\partial{\\beta}} = 0\n",
    "$$\n",
    "\n",
    "Ce qui nous donne:\n",
    "\n",
    "$$\n",
    "\\iff\\frac{\\partial{}}{\\partial{\\beta}} ((y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta) = 0\\\\\n",
    "\\iff\\frac{\\partial{}}{\\partial{\\beta}} (y^Ty - 2\\beta^TX^Ty + \\beta^TX^X\\beta + \\lambda\\beta^T\\beta) = 0\\\\\n",
    "\\iff -2X^Ty + 2X^TX\\beta + 2\\lambda\\beta = 0\\\\\n",
    "\\iff -X^Ty + (X^TX + \\lambda)\\beta = 0\\\\\n",
    "\\iff \\beta = \\frac{X^Ty}{X^TX + \\lambda}\n",
    "$$\n",
    "\n",
    "#### LASSO\n",
    "\n",
    "La méthode régularisation de LASSO permet d'appliquer une pénalité $||\\beta||_1$ à la méthode de régularisation ce qui donne:\n",
    "\n",
    "\n",
    "$$\n",
    "LASSO(\\beta) =||y-X\\beta||^{2}_{2} + \\lambda||\\beta||_1\n",
    "$$\n",
    "\n",
    "La fonction de pénalité est une régularisation $\\mathcal{L}_1$[Tib96].\n",
    "\n",
    "L'avantage d'utiliser la régularisation de LASSO est de l'utiliser dans le cas où $N < P$. La régularisation $\\mathcal{L}_1$ s'adapte mieux que la régularisation $\\mathcal{L}_2$ sur ce type de données. Hors il n'est pas adapté dans le cadre de fortes corrélations entre les variables ou si $P$ est très grand par rapport à $n$.\n",
    "\n",
    "Nous pouvons faire utiliser les deux régularisation en même temps en utilisant Elsatic NET[Hui03]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réferences des questions 2.9 et 2.10\n",
    "---\n",
    "\n",
    "[^BV04] **Convex Optimization.**\n",
    "        Stephen Boyd and Lieven Vandenberghe.\n",
    "        *Cambridge University Press, New York, NY, USA* 2004.\n",
    "\n",
    "[^TiK43] **On the stability of inverse problems**\n",
    "    A. N. Tikhonov\n",
    "    *Doklady Akademii Nauk SSSR* vol. 39, no. 5, pp. 195–198, 1943.\n",
    "\n",
    "[^Tib96] **Regression shrinkage and selectionvia the lasso.**\n",
    "        R. Tibshirani.\n",
    "        *Journal ofthe Royal Statistical Society* Series B, 58(1):267–288, 1996.\n",
    "        \n",
    "[^Hui03] **Regularization and Variable Selection via theElastic Net**\n",
    "    Hui Zou and Trevor Hastie\n",
    "    *Department of Statistics, Stanford University*, 2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.11\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ci-dessous la classe qui permet d'utiliser la méthode des moindres carrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class least_square:\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        self.beta = np.zeros(X.shape[1])\n",
    "        self.eps = np.random.randn(X.shape[0])\n",
    "        \n",
    "    def run(self):\n",
    "        self.y = self.X.dot(self.beta) + self.eps \n",
    "        for i in range(self.y.shape[0]):\n",
    "            r = (self.y[i] - self.beta.dot(self.X[i,:]))\n",
    "            self.eps[i] = r**2\n",
    "        return self.y\n",
    "    \n",
    "    def error(self):\n",
    "        somme = 0\n",
    "        for i in range(self.y.shape[0]):\n",
    "            r = (self.y[i] - self.beta.dot(self.X[i,:]))\n",
    "            self.eps[i] = r\n",
    "            somme += r**2\n",
    "        print(\"sum: %f\" % (somme))\n",
    "        self.y = self.X.dot(self.beta) + self.eps\n",
    "        return self.y\n",
    "        \n",
    "    def compute_SSE(self):\n",
    "        return np.linalg.norm(self.y - self.X.dot(self.beta), ord=2)**2\n",
    "\n",
    "    def tikhonov_reg(self, alpha):\n",
    "        self.beta = np.linalg.inv(np.transpose(self.X).dot(self.X) + np.identity(self.X.shape[1]) * alpha).dot(np.transpose(self.X).dot(self.y))\n",
    "        self.y = self.X.dot(self.beta) + self.eps\n",
    "        return self.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 8.88219239721786\n",
      "after: 36.236214\n",
      "after: 36.236214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5d9efe0d50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARPklEQVR4nO3df4wc9XnH8c8HbBKRklwSu0mKwQcqMqCChHWK3NKkEkQFUgStkkog04YmlRtVrTarSojI/5wqWVUaKderEjWyaKtUXENaWhRa4fKjBCX9A5q1Y+yA7YQg24BIuLS69IelYNdP/5hZbu+4vdtjZ3fn8b5f0mp3vrvMPIxPn5t7duY7jggBAPI6b9QFAAD6Q5ADQHIEOQAkR5ADQHIEOQAkt2EUG920aVNMTk6OYtMAkNb+/ft/HBGbl4+PJMgnJyfVarVGsWkASMv2iZXGaa0AQHIEOQAkR5ADQHIEOQAkR5ADQHIEeQbLJzZjojMAHQjyujs0LR1oLoZ3RLF8aHqUVQGoEYK8ziKk0wvSsdnFMD/QLJZPL3BkDkDSiC4IQo9saftM8frYbPGQpG2NYtweXW0AaoMj8rrrDPM2QhxAB4K87trtlE6dPXMAY48gr7POnvi2hnTn2eK5s2cOYOzRI68zW9o4sbQn3m6zbJygvQJAEkFef9dOF0fe7dBuhzkhDqBEayWD5aFNiAPoQJADQHIEOQAkR5ADQHIEOQAkR5ADQHKVBLntCdsP2j5q+4jtX6xivQCAtVV1HvmspH+JiI/bvkDShRWtFwCwhr6D3Pa7JH1Y0t2SFBGvS3q93/UCAHpTRWvlMknzkv7a9nds32f7Hcs/ZHuX7Zbt1vz8fAWbBQBI1QT5BknbJf1FRFwn6X8l3bv8QxGxNyKmImJq8+bNFWwWACBVE+QvS3o5Ip4plx9UEewAgCHoO8gj4oeSXrK9rRy6UdLz/a4XANCbqs5a+UNJc+UZKy9K+p2K1gsAWEMlQR4RByVNVbEuAMD6cGUnACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcgQ5ACRHkANAcpUFue3zbX/H9j9XtU4AwNqqPCJvSDpS4foAAD2oJMhtb5H0a5Luq2J9AIDeVXVE/meS7pF0ttsHbO+y3bLdmp+fr2izAIC+g9z2rZJei4j9q30uIvZGxFRETG3evLnfzQIASlUckV8v6TbbxyU9IOkG2/dXsF4AQA/6DvKI+GxEbImISUl3SHoyIu7quzIAQE84jxwAkttQ5coi4ilJT1W5TgDA6jgiB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4DkCHIASI4gB4Dk+g5y25fY/obt520/Z7tRRWEAgN5sqGAdZyT9UUQcsH2RpP22H4+I5ytYNwBgDX0fkUfEqxFxoHz935KOSLq43/UCAHpTaY/c9qSk6yQ9s8J7u2y3bLfm5+er3CwAjLXKgtz2z0j6B0mfiYj/Wv5+ROyNiKmImNq8eXNVmwWAsVdJkNveqCLE5yLiH6tYJwCgN1WctWJJfynpSER8of+SehCx+jIAjJEqjsivl/Rbkm6wfbB8fLSC9a7s0LR0oLkY3hHF8qHpgW0SAOqs79MPI+LfJLmCWnrZmHR6QTo2WyxvnylC/NistK1RvO/hlAIAdVHFeeTDYxfhLRXh3Q70bY1inBAHMIbyXaLfGeZthDiAMZYvyNs98U6dPXMAGDO5grwd4u2e+J1ni+djs4Q5gLGVr0e+cWJpT7zdZtk4QXsFwFjKFeSSdO300rNT2mFOiAMYU7laK23LQ5sQBzDGcgY5AOANBDkAJEeQA0ByBDkAJEeQA0ByBDkAJEeQA8CgzM1Jk5PSeecVz3NzA9lMvguCACCDuTlp1y7p1Kli+cSJYlmSdu6sdFMckQMYK0O7wdju3Ysh3nbqVDFeMYIcwNiYnpaay24w1mwW45U7eVLLf0dEOV41ghzAWIiQFhak2dnFMG82i+WFheqPzKff+QU1NfNGmIekpmY0/c7qb21MjxzAWLClmXKy1NnZ4iFJjUYxXuWUTRHSwo6bNfvolZKkGTXV1Ixm9Rk1dhyt/K6UjhHM4T01NRWtVmvo2wWAiOIkkrazZwcz716E1Lzl6BthLkmNm45qZt+Vb3l7tvdHxNTycVorAMZGu53SqTmge9LY0sy+K5eM9RPiqyHIAYyFzp54o1EciTcaS3vmg9hep0H90iDIAYwFW5qYWNoTn5kplicqvsHYsH9p8GUngLExPf3mG4xV/UVne70r/dKQqv+lIfFlJ4BMlp/uUfXpHxWruly+7ASQ26Fp6cCyq3kONIvx9RjapZ3DuyslQQ6g/iKk0wvSsdnFMD/QLJZPr+NqnkPT0t/eIk1uLSey2losr/eXQc3QIwdQf7a0vWwyH5stHpK0rVGM93KoGyEdflryo9KHJN0v6UMnJZ+UDku6pt5tmtVwRA4gh84wb+s1xNv//e4j0j5Jt0iaK5/3qRhPGuISQQ4gi3Y7pdOBdZ7Ld/Kl4ki80/3leGIEOYD66+yJb2tId54tnjt75r249BLprmVjd5XjiRHkAOrPljZOSHGT9LGHpPPPL57jpmK81x75nqsW2yk7tdhm2XPVQM9eGTSCHEAOh6+Qdn1TOnGyCN0TJ4vlw1f09t/b0jU7ivD/1qXF8rcuLZav2ZG6R84FQQBymJwsbpe23Nat0vHjva8n2UVFnQZ6QZDtm20fs/2C7XurWCcALNHtzjrrvePOsK7SGaK+g9z2+ZK+pKLTdLWkO21f3e96AWCJSy9d33gdDOkq0iqOyD8o6YWIeDEiXpf0gKTbK1gvACzas0e68MKlYxdeWIzXUVVTCvSgiiC/WFLnSZgvl2NL2N5lu2W7NT8/X8FmAYyVnTulvXuLnrhdPO/dW4zXTVVTCvRoaJfoR8ReSXul4svOYW0XwDlk5856BvdyVUwpsA5VHJG/IqnzbPot5RgAjK9+pxRYhyqC/NuSrrB9me0LJN0h6eEK1gsAeVUxpUCP+g7yiDgj6Q8kPSrpiKS/i4jn+l0vAKRV1ZQCPaqkRx4Rj0h6pIp1AUB67SkFOnvi7TZLr1MKrAPzkQPAIFw7/eYbhNa4Rw4AWMmQriIlyAEguZxBPjdXTKBz3nnF89zcqCsCgJHJ1yOfm5N27ZJOnSqWT5wolqUcFwoAQMXyHZHv3r0Y4m2nThXjAM5pQ5qDKp18QV7VVJYAUpmelprL5qBqNovxcZcvyDNOZQmgLxHSwoI0O7sY5s1msbxQ/RxU6eTrke/Zs7RHLtV7KksAfbOlmfJ6mtnZ4iFJjUYxfg7cG6Iv+Y7IM01lCaAynWHeRogX8gW5VIT28ePS2bPFMyEOnPPa7ZROzcHMQZVOziAHMFY6e+KNRnEM12gs7ZmPs3w9cgBjx5YmJpb2xNttlonq56BKxzGCX2VTU1PRarWGvl0AuXXOQbXS8rnO9v6ImFo+TmsFQBpDmoMqnZRBPnZXdzG3DIBVpAvysbu6qz23zIkTxf9se24ZwhxAKVWQj+XVXcwtA2ANqc5aGcuru5hbBsAaUh2RS2N4dRdzywBYQ7ogH7uru/bsKeaS6cTcMgA6pArysby6i7llAKwhXY98LK/u2rmT4AbQVcorO8f96i4A4+mcurKTq7sAYFHKIAcALCLIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkiPIASA5ghwAkusryG1/3vZR24dsP2R7oqrCAAC96feI/HFJvxAR10r6nqTP9l8SAGA9+gryiHgsIs6Ui09L2tJ/SQCA9aiyR/5JSfu6vWl7l+2W7db8/HyFmwWA8bbmfOS2n5D0/hXe2h0RXy8/s1vSGUldb+0eEXsl7ZWKaWzfUrUAgDdZM8gj4iOrvW/7bkm3SroxRjG5OQCMub7uEGT7Zkn3SPqViDhVTUkAgPXot0f+RUkXSXrc9kHbX66gJgDAOvR1RB4RP19VIeiOW9sBWA1Xdtbc9LTUbBbhLRXPzWYxDgASQV5rEdLCgjQ7uxjmzWaxvLCwGO4AxltfrRUMli3NzBSvZ2eLhyQ1GsU47RUAkuRRnDE4NTUVrVZr6NvNKkI6r+Nvp7NnCXFgHNneHxFTy8dprdRcu53SqbNnDgAEeY119sQbjeJIvNFY2jMHAHrkNWZLExNLe+LtnvnEBO0VAAV65AlwHjkAiR55astDmxAH0IkgB4DkCHIASI4gB4DkCHIASI4gB4DkRnL6oe15SSf6WMUmST+uqJxBqXuNda9PosYq1L0+iRrXY2tEbF4+OJIg75ft1krnUtZJ3Wuse30SNVah7vVJ1FgFWisAkBxBDgDJZQ3yvaMuoAd1r7Hu9UnUWIW61ydRY99S9sgBAIuyHpEDAEoEOQAklyLIbX/e9lHbh2w/ZHuiy+eO2z5s+6Dtoc6Tu44ab7Z9zPYLtu8dYn2/afs522dtdz2NasT7sNcaR7IPy22/x/bjtr9fPr+7y+f+r9yHB20/PIS6Vt0ntt9m+2vl+8/Ynhx0TW+hxrttz3fst98dcn1/Zfs129/t8r5t/3lZ/yHb24dZ36oiovYPSb8qaUP5+nOSPtflc8clbaprjZLOl/QDSZdLukDSs5KuHlJ9V0naJukpSVOrfG6U+3DNGke5D8vt/6mke8vX967ys/g/Q6xpzX0i6fclfbl8fYekrw3537aXGu+W9MVR/OyV2/+wpO2Svtvl/Y9K2ifJknZIemZUtS5/pDgij4jHIuJMufi0pC2jrGclPdb4QUkvRMSLEfG6pAck3T6k+o5ExLFhbOut6rHGke3D0u2SvlK+/oqkXx/itrvpZZ901v2gpBvtoc5sP+p/tzVFxDcl/ecqH7ld0t9E4WlJE7Y/MJzqVpciyJf5pIrfiisJSY/Z3m971xBrWq5bjRdLeqlj+eVyrE7qsg+7GfU+fF9EvFq+/qGk93X53Nttt2w/bXvQYd/LPnnjM+UBx08kvXfAda24/VK3f7ePlW2LB21fMpzSejbqn72uanPPTttPSHr/Cm/tjoivl5/ZLemMpLkuq/nliHjF9s9Ketz20fK3bJ1qHJhe6uvByPfhqK1WY+dCRITtbufvbi334+WSnrR9OCJ+UHWt55h/kvTViPip7d9T8RfEDSOuKYXaBHlEfGS1923fLelWSTdG2bBaYR2vlM+v2X5IxZ9zlYVQBTW+IqnzKGNLOTaU+npcx0j3YQ8Gug+l1Wu0/SPbH4iIV8s/q1/rso72fnzR9lOSrlPRIx6EXvZJ+zMv294g6V2S/mNA9axkzRojorOe+1R8H1EnA//Ze6tStFZs3yzpHkm3RcSpLp95h+2L2q9VfPm44rfPo6pR0rclXWH7MtsXqPjSaeBnNPRq1PuwR6Pehw9L+kT5+hOS3vRXhO13235b+XqTpOslPT/AmnrZJ511f1zSk90OiEZV47J+822Sjgyxvl48LOm3y7NXdkj6SUebbbRG/W1rLw9JL6joTR0sH+1v339O0iPl68tVfBP+rKTnVPypXqsaY/Gb7++pODobWo2SfkNFT++nkn4k6dEa7sM1axzlPiy3/V5J/yrp+5KekPSecnxK0n3l61+SdLjcj4clfWoIdb1pn0j6YxUHFpL0dkl/X/6c/ruky4e533qs8U/Kn7tnJX1D0pVDru+rkl6VdLr8OfyUpE9L+nT5viV9qaz/sFY5+2vYDy7RB4DkUrRWAADdEeQAkBxBDgDJEeQAkBxBDgDJEeQAkBxBDgDJ/T919tDemgzwUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_features = 2\n",
    "n_features_info = 2\n",
    "n_samples = 5\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "l = least_square(X)\n",
    "Y = l.run()\n",
    "\n",
    "print(\"before: %s\" % l.compute_SSE())\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(X[:,0], X[:,1],c='r', marker='o')\n",
    "\n",
    "ax.scatter(X[:,0], Y,c='b', marker='x')\n",
    "\n",
    "#print(l.compute_SSE())\n",
    "\n",
    "after_reg = l.tikhonov_reg(0.1)\n",
    "print(\"after: %f\" % l.compute_SSE())\n",
    "\n",
    "after_reg = l.tikhonov_reg(0.1)\n",
    "print(\"after: %f\" % l.compute_SSE())\n",
    "\n",
    "ax.scatter(X[:,0], after_reg,c='orange', marker='x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
