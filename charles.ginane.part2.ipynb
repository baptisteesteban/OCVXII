{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Régularisation de problèmes mal posés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principe\n",
    "\n",
    "#### Principe - cas général\n",
    "\n",
    "Le but de la résolution de la méthode analytique est de pouvoir séparer au mieux les données en formant plusieurs groupes.\n",
    "On recherche $y \\in R$ qui résulte de la séparation des données de $x \\in R^{(P)}$ symbolisant l'entrée. Afin de nous aider à définir la solution, nous allons prendre la matrice $X$ de dimension $N \\times P$ où $N$ représente le nombre d'éléments à partager. Nous allons en déduire $y$ à l'aide de la formule suivante:\n",
    "$$\n",
    "y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "où\n",
    "* $\\beta$ est vecteur de paramétre qui représente les éléments de $x$.\n",
    "* $\\epsilon$ qui représente les résidu (différence entre le point et la droite $y$) aussi appeler erreurs de prédiction.\n",
    "\n",
    "On peut calculer la SSE (Sum of Squared Errors) qui représente la somme de tous les résidus.\n",
    "\n",
    "$$\n",
    "SSE(\\beta) = \\sum_{i}^{N}((y_i - x_i^T\\beta)^2)\\\\\n",
    "SSE(\\beta) = (y - X\\beta)^T(y - X\\beta)\\\\\n",
    "SSE(\\beta) = ||y - X\\beta||^2_2\n",
    "$$\n",
    "\n",
    "#### Exemple avec la régression linéaire à 2 dimensions\n",
    "\n",
    "Soit deux ensembles $X$ et $Y$ de taille $n$ où $\\{x_1, x_2, ..., x_n\\}$ et $\\{y_1, y_2, ... , y_n\\}$. $X$ et $Y$ représente des points dans un domaine à 2 dimensions.\n",
    "\n",
    "on souhaite construire une droite qui séparerait au mieux les deux ensembles $X$ et $Y$.\n",
    "\n",
    "La méthode des moindres carrées nous permet d'obtenir une droite d'équation du type $y = ax + b$.\n",
    "\n",
    "Comme montrer au dessus, nous allons calculer la SSE de nos points. La SSE va nous permettre de retrouver les coefficients $a$ et $b$ ppur notre droite objectif $y$.\n",
    "Ci-dessous la SSE:\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i = 0}^{n}{(y_i - (ax_i + b))^2}\n",
    "$$\n",
    "\n",
    "Afin d'obtenir la droite pour séparer nos éléments, il faut trouver les valeurs de $a$ et de $b$ à l'aide de la formule de la $SSE$.\n",
    "\n",
    "#### Démonstration\n",
    "\n",
    "**Cherchons b**\n",
    "\n",
    "On a $$SCE = \\sum_{i = 0}^{n}{(y_i - (ax_i + b))^2} = \\sum_{i = 0}^{n}{(-ax_i - b + y_i)^2}$$\n",
    "\n",
    "Nous calculons la dérivée partielle en fonction de $b$. Nous considérons $a$ comme une constante\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{SSE}}{\\partial{b}} = 0 \\iff -2 \\sum_{i = 0}^{n}{(-ax_i - b + y_i)} = 0\n",
    "$$\n",
    "\n",
    "Nous séparons les deux termes de la somme\n",
    "\n",
    "$$\n",
    "\\iff \\sum_{i = 0}^{n}{(-ax_i - b)} + \\sum_{i = 0}^{n}{y_i} = 0\\\\\n",
    "\\iff \\sum_{i = 0}^{n}{(-ax_i)} - \\sum_{i = 0}^{n}{b} + \\sum_{i = 0}^{n}{y_i} = 0\\\\\n",
    "\\iff \\sum_{i = 0}^{n}{(-ax_i)} - nb + \\sum_{i = 0}^{n}{y_i} = 0\\\\\n",
    "\\iff -nb = a\\sum_{i=0}^{n}{x_i} - \\sum_{i = 0}^{n}{y_i}\\\\\n",
    "\\iff b = -a\\frac{\\sum{x_i}}{n} + \\frac{\\sum{y_i}}{n}\n",
    "$$\n",
    "\n",
    "On sait que $\\frac{\\sum{x_i}}{n}$ équivaut à la moyenne de x donc $\\frac{\\sum{x_i}}{n} = \\overline{x}$. De même pour $\\frac{\\sum{y_i}}{n} = \\overline{y}$. Donc\n",
    "\n",
    "$$\n",
    "b = -a\\overline{x} + \\overline{y}\n",
    "$$\n",
    "\n",
    "**Cherchons a**\n",
    "\n",
    "Avant de calculer la dérivée partielle de $SSE$ en fonction de $a$, nous allons remplacer $b$ dans l'expression de $SSE$\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i = 0}^{n}{(-ax_i + a\\overline{x} - \\overline{y} + y_i)^2}\\\\\n",
    "SSE = \\sum_{i = 0}^{n}{(-a(x_i - \\overline{x}) + (y_i - \\overline{y}))^2}\n",
    "$$\n",
    "\n",
    "On reconnait une identité remarquable du type $(a-b)^2 = a^2 - 2ab + b^2$\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=0}^{n}({a^2(x_i - \\overline{x})^2}) + 2\\sum_{i=0}^{n}({-a(x_i - \\overline{x})(y_i - \\overline{y})}) + \\sum_{i=0}^{n}(y_i - \\overline{y})^2\n",
    "$$\n",
    "\n",
    "On résoud la dérivée partielle de la $SCE$ en fonction de $a$ telle que $\\frac{\\partial{SCE}}{\\partial{a}} = 0$\n",
    "\n",
    "$$\n",
    "\\iff \\sum_{i=0}^{n}({2a(x_i - \\overline{x})^2}) - 2 \\sum_{i=0}^{n}({(x_i - \\overline{x})(y_i - \\overline{y})})\\\\\n",
    "\\iff a = \\frac{2\\sum_{i=0}^{n}({(x_i - \\overline{x})(y_i - \\overline{y})}}{2\\sum_{i=0}^{n}({(x_i - \\overline{x})^2})}\\\\\n",
    "$$\n",
    "\n",
    "Or $\\sum_{i=0}^{n}({(x_i - \\overline{x})(y_i - \\overline{y})} = \\sigma_{xy}$ et $\\sum_{i=0}^{n}({(x_i - \\overline{x})} = \\sigma_x$ donc\n",
    "\n",
    "$$\n",
    "a = \\frac{\\sigma_{xy}}{\\sigma_{x}^2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préambule\n",
    "\n",
    "Comme vu à la question 2.9, la méthode des moindres carrées permet de séparer des ensembles différents au mieux possible. Nous avons vu la notion de $SSE$ qui était la somme des résidus. Afin de pouvoir trouver la séparation optimale, il faut trouver un moyen de minimiser la $SSE$. Afin de pouvoir minimiser la $SSE$ au mieux possible, nous allons utiliser des méthodes de régularisation et de pénalité. Nous allons présentée les méthodes de Tikhonov et de LASSO, leurs points communs et leurs différences.\n",
    "\n",
    "On rappelle la $SSE$:\n",
    "\n",
    "$$\n",
    "SSE(\\beta) = ||y - X\\beta||^2_2\n",
    "$$\n",
    "\n",
    "où\n",
    "* $\\beta$ est notre ensemble\n",
    "* $y$ notre fonction objectif\n",
    "* $X$ les points\n",
    "\n",
    "On définit une pénalité dans le domaine du machine learning (ML) par\n",
    "\n",
    "$$\n",
    "Pen(\\beta) = \\mathcal{L}(\\beta) + \\lambda \\Omega(x)\n",
    "$$\n",
    "\n",
    "où\n",
    "* $\\mathcal{L}(\\beta)$ est la fonction de regression (fonction perte en ML)\n",
    "* $\\lambda$ est le paramètre de régularisation\n",
    "* $\\Omega(\\beta)$ est la fonction de penalité\n",
    "\n",
    "Les deux méthodes présentent en dessous utilisent deux fonctions de pénalité différentes.\n",
    "\n",
    "### Les méthodes\n",
    "#### Tikhonov\n",
    "\n",
    "Tikhonov (de son vrai non Andreï Nikolaïevitch Tikhonov) est un mathématicien russe. Il est connu pour avoir prouver la régularisation qui porte son nom. Le but de cette régularisation est d'appliquer une régularisation $\\mathcal{L}_2$ comme fonction de pénalité. Cette régularisation s'appelle aussi Ridge.\n",
    "\n",
    "$$\n",
    "Ridge(\\beta) = ||y-X\\beta||^{2}_{2} - \\lambda ||\\beta||^2_2\n",
    "$$\n",
    "\n",
    "Afin de pouvoir trouver le $\\beta$ minimal, il suffit de résoudre:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{Ridge}}{\\partial{\\beta}} = 0\n",
    "$$\n",
    "\n",
    "Ce qui nous donne:\n",
    "\n",
    "$$\n",
    "\\iff\\frac{\\partial{}}{\\partial{\\beta}} ((y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta) = 0\\\\\n",
    "\\iff\\frac{\\partial{}}{\\partial{\\beta}} (y^Ty - 2\\beta^TX^Ty + \\beta^TX^X\\beta + \\lambda\\beta^T\\beta) = 0\\\\\n",
    "\\iff -2X^Ty + 2X^TX\\beta + 2\\lambda\\beta = 0\\\\\n",
    "\\iff -X^Ty + (X^TX + \\lambda)\\beta = 0\\\\\n",
    "\\iff \\beta = \\frac{X^Ty}{X^TX + \\lambda}\n",
    "$$\n",
    "\n",
    "#### LASSO\n",
    "\n",
    "La méthode régularisation de LASSO permet d'appliquer une pénalité $||\\beta||_1$ à la méthode de régularisation ce qui donne:\n",
    "\n",
    "\n",
    "$$\n",
    "LASSO(\\beta) =||y-X\\beta||^{2}_{2} + \\lambda||\\beta||_1\n",
    "$$\n",
    "\n",
    "La fonction de pénalité est une régularisation $\\mathcal{L}_1$.\n",
    "\n",
    "\n",
    "### Différences\n",
    "\n",
    "La différence majeure qui se fait au sein de la méthode régulariation. LASSO est plus utilisé sur des gros jeux de données contrairement à Tikhonov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
